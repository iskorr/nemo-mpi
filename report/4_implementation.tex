\chapter{Implementation}

This chapter will cover the actual implementation of the project. The whole process of implementation was split into two parts: creation of the MPI Layer and clustering implementation within the Mapper class.

\section{MPI Layer}

The MPI Layer, as mentioned before, operates, based on the two main entities - \textbf{MasterSimulation} and \textbf{WorkerSimulation} classes.

\emph{MasterSimulation} is responsible for clustering and separation of the user input, distribution of this information to the corresponding workers and synchronisation of workers during simulation timesteps. It is the core class of the simulation, as it receives original network parameters, specified by the user.

\emph{WorkerSimulation} (sometimes regarded as nodes) is running the simulation and communicates with Master, to receive user-input information (initialisation parameters), and with other Workers during simulation. This is an auxiliary class, that is fully responsible for the local NeMo simulation running on its cluster.

\subsection{Basic Distributed Platform}

The construction of the basic platform focused on establishing the communication channel between the master simulation and the worker ones. This meant MPI initialisation on both sides, followed by an exchange messages with commonly known communication tags.

The earliest version of the distributed system, that implemented the first design step of building a distributed simulation, consisted of a master, sending out one integer (in this case it was number of neurons to be simulated), that was received by all workers. Once they received this message, self-initialisation of all parameters of the simulation followed, with consequent asynchronous completion after a set number of steps.

\subsubsection{Initial parameters distribution}

Having this initial setup ready, implementation of the second step of distributed version creation followed.
The master simulation, after having both Configuration and Network passed to it, did a basic division of the neurons per worker - distributed those uniformly (until the Mapper is fully implemented):
\begin{equation}N_{per\ worker} = \frac{N_{neurons}}{N_{workers}}\end{equation}
The translation of global id to a local one implemented within mapper at that stage is also simple:
\begin{equation}ID_{global} = ID_{local} + (Rank_{worker}-1)*N_{per\ worker}\end{equation}

\begin{figure}[h]
\begin{center}
\includegraphics[scale = 0.5]{images/distribution.png}
\end{center}
\caption{Parameters distribution}
\end{figure}

Once the worker set of parameters needed for initialisation was finalised, those need to be encoded and sent out to the workers. This is done through a set of parsing methods - a pair encode-decode for each type of the data. If the data sent is heterogeneous, i.e. more than one primitive datatype is present, the information is encoded into a string of values.

Parsing class was added for the encoding and decoding capabilities - it is accessible by both Master and Worker classes. It provides set of methods for encoding and parsing messages for further use during MPI calls.

The whole distribution stage can be split into 4 distinct steps:

\begin{enumerate}
\item{\textbf{Mapper distribution}}

Mapper has to be encoded with all information present inside - all workers must have the same mapping to ensure correctness of later communication. The data sent is split into a number of strings corresponding to the number of workers and broadcasted for the consequent reconstruction on the sub-simulation.

\item{\textbf{Configuration distribution}}

Configuration is encoded into a string that is also broadcasted to all workers. The information passed does not change, therefore, there is no need to iterate through each master-worker channel.

\item{\textbf{Neuron distribution}}

Once the mapping is set up, the neurons are distributed according to it. Firstly, the worker receives the number of neurons, and then MPI receive method is looped to gather neuronal data. Every string received is decoded, the global index is mapped locally, and, once it is done, a new neuron with this set of parameters is added to the local network.

\item{\textbf{Synapse distribution}}

The last part of the distribution stage, synaptic distribution, is also the most complex - the internal (both source and target are on the same node) synapses are sent as strings to the corresponding worker, whereas external ones need indication for the external target or source. This is done through assigning a negative value to them:
\begin{equation}Value_{external} = -(Value_{internal}+1)\end{equation}

Worker simulations have two data structures for storing incoming and outgoing synapses. The latter is built simply as an array of pairs - local id of a neuron and a vector of targets. The former, however, needs to store the synapse data - local target, weight, delay, plasticity - so it is structured as a vector of vectors of structs. These two are populated as the worker receives and decodes the synaptic data for external synapses.

Note, that even though the external synaptic structure was implemented, it is not put to use at this stage, as there is no spike delivery integrated between sub-simulations within the system at this point.
\end{enumerate}

Once all parameters are received, workers set up their simulations and notify master when it is done. When master receives confirmation from all workers, the simulation may commence, through broadcasting the step signal across the network. 

\subsection{Communication Channels Integration}

At this point the system has a fully operational master-worker distribution channel. Next step is implementation of an inter-worker communication channel used for spike delivery.

First of all, the sources need to be identified, to acquire knowledge of the corresponding targets and their worker location. As given by the NeMo architecture, the simulation step function returns IDs of the neurons fired during this simulation step. These are then used to derive any outgoing spikes, i.e. going through external synapses. The worker step function is done in similar way to the original NeMo implementation, it could be split into three sub-steps\cite{AndreasK.Fidjeland2009}:

\begin{enumerate}
\item {\textbf{Enqueuing Spikes (Prefire)}} 

This is a stage during which the data about incoming currents is collected and passed to the fire function.
Within MPI Simulation, function $enqueueIncomingSpikes()$ provides this functionality, by taking the incoming current data and passing it to the NeMo simulation $step()$ function.

\item {\textbf{Local Simulation Step (Fire)}}

This function determines which neurons have fired by updating the neuron values based on the data collected during prefire step. Original NeMo simulation $step()$ function is used as \emph{Fire} stage returning the vector of fired neurons. The fired neurons are passed to the postfire function.

\item {\textbf{Distributing Spikes (Postire)}}

The final stage of the simulation step, during which the spikes are distributed across the network. Local distribution is done within NeMo step, external is done through series of MPI calls by Worker simulations.
\end{enumerate}

Once a Worker has finished its step, it sends notification to the Master, who, after receiving confirmation from all Workers, advances the global simulation one step further.

After finalisation of overall communication structure, the external delivery design needs to be specified. In the following sections the spike delivery system is given in details.

\subsubsection{Spike Enqueuing}

In order to provide the capacity of storing the incoming spikes, WorkerSimulation class has two data structure for storing those: dequeuing $incoming\_queue$ and priority $delay\_queue$.

The purpose of $incoming\_queue$ is to collect global IDs of all neurons, that have a target on this node. It is populated during the later distribution step.

The $delay\_queue$ is created to store the spikes depending on their delay values. Delay represents number of simulation steps that need to commence before this spike reaches the target node.

\begin{figure}[h]
\begin{center}
\includegraphics[scale = 0.5]{images/spike_enqueuing_scheme.png}
\end{center}
\caption{Representation of spike enqueuing}
\end{figure}

Taking this into account, $enqueueIncomingSpikes()$ function can be split into two steps:

During first step, it continously dequeues $incoming\_queue$ (1), while it is not empty, looks up the targets connected to this source in Worker's $incomingSynapses$ structure, and puts them into $delay\_queue$ depending on their delays (2). 

The second step commences, after the delay sorting has been resolved. A vector of floats (incoming currents) $stimulus$ is initialised with size corresponding to the number of neurons on the node and values set to 0.0. The head of the delay queue (delay = 1) is poped and iterated through (3). Each entry consists 2 values: target and weight. Using those, the current vector is updated: for each target a synapse weight is added to its current value (4).

The resulting vector is then passed to the local NeMo simulation $step(stimulus)$ function.

\subsubsection{Spike Distribution}

The simulation step returns a vector $fired$ of local neuron IDs, indicating the neurons that have fired. After taking this vector as a parameter, $distributeOutgoingSpikes(fired)$ performs external distribution of spikes.

\begin{figure}[h]
\begin{center}
\includegraphics[scale = 0.5]{images/spike_distribution_scheme.png}
\end{center}
\caption{Representation of spike distribution}
\end{figure}

This function can also be split into two steps:

First step: the vector of fired neurons (1) is iterated through, IDs are looked up in the $outgoingSynapses$ structure. As each ID has a vector of corresponding target nodes (2), this vector is iterated through (3), and a non-blocking MPI message with the global ID of a neuron is sent to the target. After the distribution is finalised, an ending message is sent to all workers.

Second step: after all non-blocking messages have been sent, function initiates a loop that enqueues (4) all incoming IDs into the $incoming\_queue$. This loop only terminates after receiving the ending messages from all other workers.

After finishing these two steps, the function returns, leading to the end of the distributed simulation step on this worker.

\section{Clustering}

Clustering implementation is dependent on the algorithm introduced by Mark Newman in 2006. This section will provide the mathematical overview of this algorithm and its implementation within Mapper class.

\subsection{Newman's Algorithm}

Major part of Newman's research concentrates on finding communities in the network structures\cite{NewmanComm}. \emph{Community structure} is a feature of a graph that shows the gathering of vertices into groups with high connectivity. This feature is best described by modularity, the measure of modular division of a particular network into subnets with high edge density.

Formula for modularity is (for a network of $n$ vertices):
\begin{equation}Q = \frac{1}{4m}\Sigma_{ij}(A_{ij} - \frac{k_{i}k_{j}}{2m})s_{i}s_{j}\end{equation}\cite{Newman2006}
where $m$ is total number of edges, $k_{i}$ and $k_{j}$ are the degrees of the corresponding neurons. Notice that $\frac{1}{4m}$ is a conventional leading factor - it is there to support previous definition of modularity, given by Newman and Girvan\cite{Newman2004}.

Newman's algorithm ("Method of Optimal Modularity"\cite{Newman2006}) is a way of determining the natural division of graph's vertices into several non-overlapping communities, based on the modularity measure. Its structure is different to the older algorithm used for finding communities\cite{NewmanAlgo}, as the latter was more scientifically focused, therefore lacked certain capabilities for implementation in the parallel computation design.

\clearpage

The step structure of the algorithm is as follows:

\begin{enumerate}
\item{\textbf{Create an adjancency matrix of the network}}

Create and populate the adjacency matrix for the network based on the information about edges.

\item{\textbf{Create and populate a modularity matrix of the network}}

Matrix $Q$ is created with the same dimensions as the adjacency one. Its values are set through using equation:

\begin{equation}Q[i][j] = A[i][j] - \frac{k_{i}k_{j}}{2m}\end{equation}

Notice that the conventional factor is abolished - it does not affect the result, so its absence will improve execution time.

\item{\textbf{Generate a leading eigenvector for the modularity matrix}}

For the resulting Q matrix the dominating eigen vector has to be generated - this is done through the use of power-iteration algorithm\cite{VonMises1929}.

Power iteration algorithm: $b_{k+1} = \frac{Ab}{||Ab||}$

\item{\textbf{Eigenvector values are used to split the network into two partitions}}

The values of eigenvector correspond to the split of this network into two parts - negative and positive values separate vertices into two groups. In case, more groups are needed, the algorithm is performed again on the partitions.

\end{enumerate}

As it can be observed Newman's algorithm is robust and relatively easy to implement. Moreover, it possesses another feature - whenever the connectivity of the network is all-to-all or close to this state, the resulting eigenvector would have only positive values - therefore, indicating an indivisible graph, i.e. a graph without a natural community separation. This certain feature could be used later for improvement - specifically for the purpose of finding an optimal number of clusters that a particular network has to be split into.

\subsection{Mapper integration}

The implementation of Newman's algorithm into the Mapper class is essentially translation of all of the algorithmic steps into the code. The focus, therefore, should be on the matrix updates and the power-iteration function.

The main difference from the actual algorithm would be the use of one matrix instead of two - this is done to prevent probable memory leaks, as testing with large scale networks showed that matrices for big number of neurons occupy too much RAM, causing the Master process to get killed. Therefore, current implementation has a threshold value of 31000 neurons, after which a simple uniform distribution is used for clustering.

This is the implementation of adjancency and modularity (both within $q\_matrix$) update.

\begin{center}
\lstset{
    caption = Code for Matrix update,
    basicstyle=\ttfamily\footnotesize\bfseries,
    frame=tb
}
\begin{lstlisting}
...
/* Parameters initialisation */
for(i = 0; i < ncount; ++i) {
	vector<synapse_id> synapses = net.getSynapsesFrom(partition[i]);
	for (j = 0; j < synapses.size(); ++j) {
		unsigned globtarget = net.getSynapseTarget(synapses[j]);
		if (auxMap[globtarget] == partcount) {
			unsigned target = backMap[globtarget];
			q_matrix[i][target]++;
			q_matrix[target][i]++;
			degrees[i]++;
			degrees[target]++;
			edges++;
		}
	}
}
for (i = 0; i < ncount; ++i) {
	for (j = 0; j < ncount; ++j) {
		if (i != j) q_matrix[i][j] = (q_matrix[i][j] - (float)(degrees[i]*degrees[j])/(2*edges));
		else q_matrix[i][j] = STRENGTH;
	}
}
\end{lstlisting}
\end{center}

In the code shown above, the first loop initialises the adjacency matrix - it is done through interaction with the Network class. For each of the synapses, source and target values are checked to be within the partition. Once proved to be correct, values for adjacency, degree of vertices and number of edges are updated.

The second loop constructs modularity matrix on top of the adjacency one. The updates are done in accordance to the modularity formula, mentioned before (4.5).
The constant STRENGTH is set for all $q\_matrix_{ii}$ objects for the purpose of better computations, as original algorithm did not permit loops.
Also, note that due to big size of all created data structures, each call for $allocateNeurons()$ is ending with a set of destructors.

Once the $q\_matrix$ values are all set, the only thing left to do - is to calculate the dominant eigen vector of this matrix:

\begin{center}
\lstset{
    caption = Power-Iteration (Von Moses) algorithm,
    basicstyle=\ttfamily\footnotesize\bfseries,
    frame=tb
}
\begin{lstlisting}
float* eigenvector = new float [ncount];
float* tmp = new float [ncount];
...
/* Parameters initialisation */
while(step < THRESHOLD) {
	norm_sq=0;
	for (i = 0; i < ncount; ++i) {
        	tmp[i] = 0;
       		for (j = 0; j < ncount; ++j) tmp[i] += q_matrix[i][j] * eigenvector[j];
		norm_sq += tmp[i]*tmp[i];
	}
   	norm = sqrt(norm_sq);
	for (i = 0; i < ncount; ++i) eigenvector[i] = tmp[i]/norm;
	step++;
}
\end{lstlisting}
\end{center}

This part of code is responsible for the eigen vector generation. Eigenvector is first set to {$1,1...1$} and then passed to the while loop for several steps of power iteration. The number of steps, THRESHOLD constant, after thorough testing, is set to 10, as this number of iteration steps provides a compromise between accuracy and heaviness of computation.

\begin{figure}[h]
\begin{center}
\includegraphics[scale = 0.4]{images/clustering_recursive.png}
\end{center}
\caption{Representation of a recursive call to the clustering function for 5 clusters}
\end{figure}

\section{Alternative Solutions}

There were a few alternative ways that the above algorithms could be implemented - however, most of them quickly proved to be insufficient for the purpose or too time-consuming to implement.

\begin{itemize}
\item{\textbf{Use of Boost MPI libraries}}

The idea of this solution was that boost MPI libraries comprise all needed functionality in an easy and well-represented way. However, use of both Boost and CMake caused numerous build fails, which in the end, outweighed the positive side. Moreover, most of the system that was implemented already at that point was written with MPICH - therefore, a change of implementation library would consume more time.

\item{\textbf{Implementation of spike receiving into enqueuing step}}

This proposal was aimed at having a wider window of simulation - by creating unblocking receives at the start, that would be filled in with the non-blocking send. However, such an implementation is leading to race conditions occuring, and, moreover, needs the starting input sent from all nodes, which could cause a significant overhead. Furthermore, a single communication window implemented in $distributeOutgoingSpikes()$ method is much easier to control and debug, whenever an MPI call fails.

\item{\textbf{Expand the limits of mapping by using both uniform and Newman clustering algorithms}}

As mentioned before, the current implementation of clustering algorithm is not fully applicable to large scale networks, mostly due to the RAM limitations. However, uniform mapping does not require that much memory, therefore, if it is done first, leaving less computationally heavy chunks for advanced clustering, it should produce faster results. The outcome however, is quite the opposite - the initial uniform distribution adds the most external synapses to the partitions - as a result, rendering most of the further clustering not as useful, especially considering setup time spent on it.
\end{itemize}
